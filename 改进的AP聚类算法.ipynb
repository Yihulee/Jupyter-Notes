{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面改进的k均值聚类貌似效果不错,我就想用AP聚类来试一试,写这个算法废了不少的劲,不过效果不怎么好,因为我发现,一旦数据增多,这个算法分的类会急剧上升,当然数据量比较小的话,结果还是不错的.这里贴一下代码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AffinityProp(object):\n",
    "    \"\"\"\n",
    "        Implementing the affinity propagation algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, similarity_matrix, max_iteration=200, num_iter=5,\n",
    "    alpha=0.5, verbose=True, print_every=100):\n",
    "        \"\"\"\n",
    "            similarity_matrix: N * N matrix containing similarities\n",
    "            max_iteration: The maximum number of iterations to perfrom for clustering\n",
    "            num_iter: num iterations without no change in the number of clusters\n",
    "            that stops the algorithm\n",
    "        \"\"\"\n",
    "        self.s = similarity_matrix # 得到相似矩阵\n",
    "        self.max_iteration = max_iteration\n",
    "        self.alpha = alpha\n",
    "        self.verbose = verbose\n",
    "        self.print_every = print_every\n",
    "\n",
    "        #  INIITALISE THE RESPONSIBILITY AND THE AVAILABILITY MATRICES\n",
    "        N, N = self.s.shape\n",
    "        self.r = np.zeros((N, N)) # r, a两个矩阵在初始的时候全部是0\n",
    "        self.a = np.zeros((N, N))\n",
    "\n",
    "    def _step(self):\n",
    "        \"\"\"\n",
    "            This is meant ot return the new Availability and repsonsibility\n",
    "            matrices for all the data points\n",
    "        \"\"\"\n",
    "        # 一次一次迭代\n",
    "        N, N = self.s.shape # s是相似矩阵\n",
    "        old_r = self.r\n",
    "        old_a = self.a\n",
    "\n",
    "        #R UPDATE STEP\n",
    "        a_plus_s = self.a + self.s # 首先两个向量相加\n",
    "        first_max = np.max(a_plus_s, axis=1) # 找到最大的\n",
    "        first_max_indices = np.argmax(a_plus_s, axis=1)\n",
    "        first_max = np.repeat(first_max, N).reshape(N, N)\n",
    "        a_plus_s[range(N), first_max_indices] = -np.inf\n",
    "        second_max =  np.max(a_plus_s, axis=1)\n",
    "        r = self.s - first_max\n",
    "        r[range(N), first_max_indices] = self.s[range(N), first_max_indices] - second_max[range(N)]\n",
    "        r = self.alpha * old_r + (1 - self.alpha) * r\n",
    "\n",
    "        # A UPDATE STEP\n",
    "        rp = np.maximum(r, 0)\n",
    "        np.fill_diagonal(rp, np.diag(r))\n",
    "        a = np.repeat(np.sum(rp, axis=0), N).reshape(N,N).T - rp\n",
    "        da = np.diag(a)\n",
    "        a = np.minimum(a, 0)\n",
    "        np.fill_diagonal(a, da)\n",
    "        a = self.alpha * old_a + (1 - self.alpha) * a\n",
    "\n",
    "        return r, a\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "            This runs the affinity propagation algorithm until convergence\n",
    "            TODO: make it into a separate file with more options for better update\n",
    "            rules and more options\n",
    "        \"\"\"\n",
    "        for i in range(self.max_iteration):\n",
    "            if self.verbose and i % self.print_every is 0:\n",
    "                print(\"processing iteration %d\" % (i, ))\n",
    "            self.r, self.a = self._step()\n",
    "\n",
    "        e = self.r + self.a\n",
    "\n",
    "        return e # 这个可以注释掉\n",
    "        N, N = e.shape\n",
    "\n",
    "        # NOTE: THIS IS ACCORDING TO THE PAPER\n",
    "        # THIS IS NOT REMOTELY RELATED TO SCIKIT LEARN\n",
    "        # SO I REALLY CANT COMPARE MY RESULTS TO SCIKIT LEARN BEYOND THIS POINT\n",
    "\n",
    "        # I will contain the index of the data point that will be an exemplar\n",
    "        # For example 40 and 55 of say 60 points may serve as the exemplars\n",
    "        '''\n",
    "        关于diag,给一个例子,可以明显看出diag用于取对角线的元素\n",
    "        numpy.diag(v, k=0)\n",
    "            Extract a diagonal or construct a diagonal array.\n",
    "        Parameters\n",
    "        v : array_like\n",
    "        If v is a 2-D array, return a copy of its k-th diagonal. If v is a 1-D array, return a 2-D\n",
    "        array with v on the k-th diagonal.\n",
    "        k : int, optional\n",
    "        Diagonal in question. The default is 0. Use k>0 for diagonals above the main diagonal,\n",
    "        and k<0 for diagonals below the main diagonal.\n",
    "        Returns\n",
    "        out : ndarray\n",
    "        The extracted diagonal or constructed diagonal array.\n",
    "        >>> x = np.arange(9).reshape((3,3))\n",
    "        >>> x\n",
    "        array( [[0, 1, 2],\n",
    "                [3, 4, 5],\n",
    "                [6, 7, 8]])\n",
    "        >>> np.diag(x)\n",
    "        array([0, 4, 8])\n",
    "        >>> np.diag(x, k=1)\n",
    "        array([1, 5])\n",
    "        >>> np.diag(x, k=-1)\n",
    "        array([3, 7])\n",
    "        >>> np.diag(np.diag(x))\n",
    "        array( [[0, 0, 0],\n",
    "                [0, 4, 0],\n",
    "                [0, 0, 8]])\n",
    "        '''\n",
    "        # temp1 = np.diag(e)\n",
    "        # temp2 = temp1 > 0\n",
    "        # temp3 = np.where(temp2)\n",
    "        # temp4 = temp3[0]\n",
    "        I = np.where(np.diag(e) > 0)[0]\n",
    "        K = len(I)\n",
    "\n",
    "        # Select all the rows of S where column_index = 40 and 55\n",
    "        c = self.s[:, I]\n",
    "        # For every data point chose the exemplar that has maximum similarity with it\n",
    "        # For example 1st data point may have maximum similarity with only 44\n",
    "        # 2nd data point may have max similarity with 55\n",
    "        # One explanation of why this may be done is to ensure that kth data\n",
    "        # point not only maximises the sum a+r but also the similarity that it\n",
    "        # has with i\n",
    "        # so every c will be either 0 or 1 (considering only 2 exemplars are there)\n",
    "\n",
    "        '''\n",
    "        >>> a = np.arange(6).reshape(2,3)\n",
    "        >>> a\n",
    "        array( [[0, 1, 2],\n",
    "                [3, 4, 5]])\n",
    "        >>> np.argmax(a)\n",
    "        5\n",
    "        >>> np.argmax(a, axis=0)\n",
    "        array([1, 1, 1])\n",
    "        >>> np.argmax(a, axis=1)\n",
    "        array([2, 2])\n",
    "        >>> b = np.arange(6)\n",
    "        >>> b[1] = 5\n",
    "        >>> b\n",
    "        array([0, 5, 2, 3, 4, 5])\n",
    "        >>> np.argmax(b) # Only the first occurrence is returned.\n",
    "        1\n",
    "        '''\n",
    "        c = np.argmax(c, axis=1)\n",
    "\n",
    "        # Make the c[exemplar_1] = 0 and c[examplar_1] = 1\n",
    "        c[I] = np.arange(0, K)\n",
    "\n",
    "        # Get back the index to the original data set\n",
    "        # say c= [0, 1, 1, 0]\n",
    "        # 0th exemplar point is 40 in the original data and 1 is 55\n",
    "        # mapping from c -> I is done like this in numpy\n",
    "        idx = I[c]\n",
    "\n",
    "        exemplar_indices = I\n",
    "        exemplar_assignments = idx\n",
    "        return exemplar_indices, exemplar_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the two similarities equal True\n",
      "Is similarities with medians equal True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "怎么说呢,这是一个很高级的矩阵计算,给定一个N*M维的矩阵,记作K,其中N代表元素的个数,M代表维数\n",
    "我们要输出一个矩阵,这个矩阵N*N大小,记作G,例如G[1, 2]记录了G中第一个元素的第二个元素的欧几\n",
    "里德距离,非常高效的算法\n",
    "'''\n",
    "\n",
    "def euclidean_distance(data, squared=True):\n",
    "    \"\"\"\n",
    "        ARGS:\n",
    "            INPUT:\n",
    "                data: A matrix of shape (N, M)\n",
    "                N -> Number of data points\n",
    "                M -> Number of dimensions\n",
    "            OUTPUT:\n",
    "                similarity: of shape N * N representing the euclidean\n",
    "                distance between the ith sample with all other samples\n",
    "                Also here the sel similarity of the data point i is set to\n",
    "                the median of the similarities with other data points\n",
    "    \"\"\"\n",
    "    # N代表数据的个数,M代表维度\n",
    "    N, M = data.shape\n",
    "    # astype返回一个data的副本,但是元素被变成了float类型\n",
    "    data = data.astype('float') # just making sure that it is float\n",
    "    data_copy = data.copy() # 获得一个data的副本\n",
    "    distance = np.zeros((N, N)) # distance是一个N*N的矩阵\n",
    "    '''\n",
    "    关于sum函数: numpy.sum(a, axis=None, dtype=None, out=None)\n",
    "    a是array_like的元素\n",
    "    axis是一个可选的参数,整数,默认的axis是None,从而a中所有的元素都被相加\n",
    "    dtype也是一个可选的参数,决定结果的类型\n",
    "    >>> np.sum([0.5, 1.5])\n",
    "    2.0\n",
    "    >>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n",
    "    1\n",
    "    >>> np.sum([[0, 1], [0, 5]])\n",
    "    6\n",
    "    >>> np.sum([[0, 1], [0, 5]], axis=0)\n",
    "    array([0, 6])\n",
    "    >>> np.sum([[0, 1], [0, 5]], axis=1)\n",
    "    array([1, 5])\n",
    "    '''\n",
    "    data_square = np.sum(np.square(data), axis=1) # 得到向量的平方和\n",
    "    # data_copy和data其实是一致的\n",
    "    data_copy_square = np.sum(np.square(data_copy), axis=1)\n",
    "    multiply = np.dot(data, data_copy.T) # 点乘,实际上是求两个矩阵相乘\n",
    "    temp = data_square[:, np.newaxis]\n",
    "    temp1 = 2 * multiply\n",
    "    '''\n",
    "    减法有一点诡异,举一个例子:\n",
    "    >>> x1 = np.arange(9.0).reshape((3, 3))\n",
    "    >>> x1\n",
    "    array( [[ 0.,  1.,  2.],\n",
    "            [ 3.,  4.,  5.],\n",
    "            [ 6.,  7.,  8.]])\n",
    "    >>> x2 = np.arrange(3.0)\n",
    "    >>> x2\n",
    "    array([ 0.,  1.,  2.])\n",
    "    >>> np.subtract(x1, x2)\n",
    "    array( [[ 0.,  0.,  0.],\n",
    "            [ 3.,  3.,  3.],\n",
    "            [ 6.,  6.,  6.]])\n",
    "    '''\n",
    "    temp3 = data_copy_square - temp1\n",
    "    '''\n",
    "    >>> np.add(1.0, 4.0)\n",
    "    5.0\n",
    "    >>> x1 = np.arange(9.0).reshape((3, 3))\n",
    "    >>> x1\n",
    "    array( [[ 0.,  1.,  2.],\n",
    "            [ 3.,  4.,  5.],\n",
    "            [ 6.,  7.,  8.]])\n",
    "    >>> x2 = np.arange(3.0)\n",
    "    >>> x2\n",
    "    array([ 0.,  1.,  2.])\n",
    "    >>> np.add(x1, x2)\n",
    "    array( [[ 0., 2., 4.],\n",
    "            [ 3., 5., 7.],\n",
    "            [ 6., 8., 10.]])\n",
    "    '''\n",
    "    temp4 = temp + temp3\n",
    "    distance = data_square[:, np.newaxis] + data_copy_square - (2 * multiply)\n",
    "\n",
    "    if not squared:\n",
    "        distance = np.sqrt(distance)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Testing the euclidean similarity with simple matrices\n",
    "\n",
    "    data = np.array([[1, 2, 3, 4], [5, 6, 7, 8]], dtype='float')\n",
    "    N, M = data.shape\n",
    "    correct_output = np.array([[0.0, 8.0], [8.0, 0.0]])\n",
    "    similarity = euclidean_distance(data, squared=False)\n",
    "    print(\"Are the two similarities equal %s\" % (np.allclose(similarity, correct_output),))\n",
    "\n",
    "    # Filling the diagonal of the euclidean distance\n",
    "    # to the median but not including the self similarity points themselves\n",
    "    # Using the masked array to find the median of the similarities\n",
    "    masked_array = np.zeros((N, N), dtype='Bool')\n",
    "    np.fill_diagonal(masked_array, True) # make the diagonals invalid\n",
    "    ma_similarity = np.ma.masked_array(similarity, masked_array)\n",
    "    np.fill_diagonal(similarity, np.ma.median(ma_similarity))\n",
    "    correct_similarity_with_median = np.array([[8.0, 8.0], [8.0, 8.0]])\n",
    "    print(\"Is similarities with medians equal %s\" % (np.allclose(similarity, correct_similarity_with_median)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def plotClusters(clusters, title):\n",
    "    \"\"\" 画图 \"\"\"\n",
    "    plt.figure(figsize=(8, 5), dpi=80)\n",
    "    axes = plt.subplot(111)\n",
    "    col=[]\n",
    "    r = lambda: random.randint(0,255)\n",
    "    for index in range(len(clusters)):\n",
    "        col.append(('#%02X%02X%02X' % (r(),r(),r())))\n",
    "    color = 0\n",
    "    for cluster in clusters:\n",
    "        cluster = np.array(cluster).T # T方法表示矩阵的转置\n",
    "        axes.scatter(cluster[0],cluster[1], s=50, c = col[color])\n",
    "        color += 1\n",
    "    plt.title(title)\n",
    "\n",
    "def computeCluster(fitable, data):\n",
    "    clusters = {}\n",
    "    num = len(fitable)\n",
    "    for node in range(num):\n",
    "        fit = list(fitable[node])\n",
    "        key = fit.index(max(fit))\n",
    "        if not key in clusters:\n",
    "            clusters[key] = []\n",
    "        point = tuple(data[node])\n",
    "        clusters[key].append(point)\n",
    "    return clusters\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    n_clusters = 3\n",
    "    centers = np.array([[1, 1], [-1, -1], [1, -1]]) + 10\n",
    "    X, _ = make_blobs(n_samples=350, n_features=2, centers=centers,\n",
    "                  cluster_std=0.4, shuffle=True, random_state=0)\n",
    "    S = -1 * euclidean_distance(X, squared=True)\n",
    "    median_value = np.median(S) * 10\n",
    "    np.fill_diagonal(S, median_value)\n",
    "\n",
    "    af_prop = AffinityProp(S)\n",
    "    #exemplar_indices, exemplar_assignments = af_prop.solve()\n",
    "    e = af_prop.solve()\n",
    "    clusters = computeCluster(e, X)\n",
    "    clusters = clusters.values()\n",
    "    print(len(clusters))\n",
    "    plotClusters(clusters, \"clusters by affinity propagation\")\n",
    "    # print(\"cluster center indices mine: %s\" % (exemplar_indices,))\n",
    "    # print(len(exemplar_assignments))\n",
    "    # print(np.unique(exemplar_assignments, return_counts=True))\n",
    "\n",
    "    # RESULTS\n",
    "    # THE INDICES OF THE EXEMPLARS IS 4 21 AND 22\n",
    "    # THERE ARE 19, 20, 21 ELEMENTS IN EACH CLUSTER INCLUDING THE EXEMPLAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在最后给出一张通过这个算法实现的非常漂亮的分类图片吧!一共大概350个点,但是如果点的数目再增加的话,分类效果就非常差了,所以我并不打算用这个聚类算法来分类我的数据.\n",
    "![ap聚类](fig_4.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
